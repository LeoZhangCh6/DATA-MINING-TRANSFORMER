{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "4753b3670bc44259a7a73f2d2f0a5a03",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2752,
    "execution_start": 1670543245129,
    "source_hash": "1653f504",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from einops import rearrange\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "b8d4569c586945548afb6d333e7cb082",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1670543247910,
    "source_hash": "4141948b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a0d32b22099b4cc8bd5315e0a4d8cc17",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "# Attention and Performer Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "020883a444c241c391f246458b181fe8",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## Softmax Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "64440fcce9354e508045fd0489c6dfc6",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "The definition/computation can be found in Attention Is All You Need Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "e85d0d19c9d74460bef147465ab84d7b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1670543247911,
    "source_hash": "75eaecce",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = dim ** -0.5 # 1/sqrt(m)\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x)\n",
    "        q, k, v = rearrange(qkv, 'b n (qkv h d) -> qkv b h n d', qkv=3, h=h)\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
    "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
    "            mask = mask[:, None, :] * mask[:, :, None]\n",
    "            dots.masked_fill_(~mask, float('-inf'))\n",
    "            del mask\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out =  self.to_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a1d96aac0f924cad84a9c0e2afec20c5",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## Performer Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "60c6a9d4-2cbf-4b68-8eb1-3e6062aef47f",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": [
     {
      "fromCodePoint": 0,
      "marks": {
       "code": true
      },
      "toCodePoint": 11,
      "type": "marks"
     },
     {
      "fromCodePoint": 118,
      "marks": {
       "code": true
      },
      "toCodePoint": 129,
      "type": "marks"
     },
     {
      "fromCodePoint": 144,
      "marks": {
       "code": true
      },
      "toCodePoint": 147,
      "type": "marks"
     }
    ],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "feature_map essentially defines performer variants. i.e. when the project says Performer-ReLU, it means to change the feature_map here to ReLU. rom is the random orthogonal matrix generator. How this is generated is described in later sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3fc3034a-1ef0-4021-8885-9df7749e780d",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": [
     {
      "fromCodePoint": 0,
      "marks": {
       "code": true
      },
      "toCodePoint": 11,
      "type": "marks"
     }
    ],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "feature_map are1-D (random) feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "99ea9db205f34d8b8d744b894742e678",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1670543247916,
    "source_hash": "f9792513",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PerformerAttention(nn.Module):\n",
    "    def __init__(self, dim, heads, r, feature_map, projection_method='gaussian', redraw=False):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = r ** -0.5 # 1/sqrt(m)\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "\n",
    "        \n",
    "        self.r = r # random feature map dimension\n",
    "        if projection_method == 'gaussian':\n",
    "            self.create_projection_matrix = create_projection_gaussian\n",
    "        elif projection_method == 'HD':\n",
    "            self.create_projection_matrix = create_projection_HD\n",
    "        self.projection_matrix = self.create_projection_matrix(r, dim//heads)\n",
    "\n",
    "        self.feature_map = feature_map # this is only 1-D random feature map\n",
    "        self.redraw = redraw\n",
    "\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        # x shape: (batch x n x (head * dim))\n",
    "        b, n, hxd, h = *x.shape, self.heads\n",
    "        d = hxd // h\n",
    "\n",
    "        qkv = self.to_qkv(x)\n",
    "        # qkv shape: (batch x n x 3 * dim)\n",
    "        q, k, v = rearrange(qkv, 'b n (qkv h d) -> qkv b h n d', qkv=3, h=h)\n",
    "        # q, k, v shape: (batch x head x n x dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Dimension Reduction\n",
    "        # self.projection_matrix shape (r x dim)\n",
    "        k_ = self.feature_map(torch.einsum('rd,bhnd->bhnr', self.projection_matrix, k)) * self.scale\n",
    "        if self.redraw:\n",
    "            self.projection_matrix = self.create_projection_matrix(r, d)\n",
    "        q_ = self.feature_map(torch.einsum('rd,bhnd->bhnr', self.projection_matrix, q)) * self.scale\n",
    "        if self.redraw:\n",
    "            self.projection_matrix = self.create_projection_matrix(r, d)\n",
    "\n",
    "        k_Tv = torch.einsum('bhir,bhid->bhrd', k_, v)\n",
    "        q_k_Tv = torch.einsum('bhnr,bhrd->bhnd', q_, k_Tv)\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Compute diagnal D Matrix (strictly positive diagnol values)\n",
    "        k_T1_L = torch.sum(k_, dim=2)\n",
    "        d = torch.einsum('bhnr,bhr->bhn', q_, k_T1_L)\n",
    "#         assert torch.all(torch.all(d >= 0)), torch.sum(d < 0)\n",
    "        d[d==0] = 1e-6\n",
    "        d = torch.diag_embed(torch.pow(d, -1))\n",
    "        out = torch.einsum('bhmn,bhnd->bhmd', d, q_k_Tv)\n",
    "\n",
    "        # Output\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out =  self.to_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0945f0576c0146d08b6c7fa012525458",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "### Orthogonal Random Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "dac4422e-871e-495a-b846-906bf31ba318",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "Two approaches are implemented here for generating Random Orthogonal matrices. i.e. random feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "77395fcb-a2c6-47d4-9771-dcb2fd6666aa",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "### HD Block Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0bdcb0e2a4ec40e7ad4ac29b4aadc615",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "We here use a single HD block here H is a Hadamard Matrix and D is a random diagonal matrix. More details can be found in the PNG Kernal paper. The catch is that dim (hyperparameter) has to be a power of 2. This is noted in the hyperparameter section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e504c501-80a4-47f0-8a71-8632d214e282",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": [
     {
      "fromCodePoint": 42,
      "ranges": [],
      "toCodePoint": 131,
      "type": "link",
      "url": "https://github.com/HazyResearch/structured-nets/blob/master/pytorch/structure/hadamard.py"
     }
    ],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "Hadamard Transformation code provided by: https://github.com/HazyResearch/structured-nets/blob/master/pytorch/structure/hadamard.py It makes use of the NumPy library. Please be careful and try NOT to use NumPy functions anywhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "e8e0b536f48d4fa4b8ad8bb86ef0ee66",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1670543247952,
    "source_hash": "63e576c5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hadamard_transform(u, normalize=False):\n",
    "    \"\"\"Multiply H_n @ u where H_n is the Hadamard matrix of dimension n x n.\n",
    "    n must be a power of 2.\n",
    "    Parameters:\n",
    "        u: Tensor of shape (..., n)\n",
    "        normalize: if True, divide the result by 2^{m/2} where m = log_2(n).\n",
    "    Returns:\n",
    "        product: Tensor of shape (..., n)\n",
    "    \"\"\"\n",
    "    _, n = u.shape\n",
    "    m = int(np.log2(n))\n",
    "    # assert n == 1 << m, 'n must be a power of 2'\n",
    "    x = u[..., np.newaxis]\n",
    "    for d in range(m)[::-1]:\n",
    "        x = torch.cat((x[..., ::2, :] + x[..., 1::2, :], x[..., ::2, :] - x[..., 1::2, :]), dim=-1)\n",
    "    return x.squeeze(-2) / 2**(m / 2) if normalize else x.squeeze(-2)\n",
    "\n",
    "def create_projection_HD(m, d, seed=0, manual_seed=False):\n",
    "    if manual_seed:\n",
    "        torch.manual_seed(current_seed)\n",
    "    \n",
    "    nb_full_blocks = int(m / d)\n",
    "    block_list = []\n",
    "    current_seed = seed\n",
    "    for _ in range(nb_full_blocks):\n",
    "        if manual_seed:\n",
    "            torch.manual_seed(current_seed)\n",
    "        random_diag = torch.randint(-1, 2, (d,))\n",
    "        hd_matrix = hadamard_transform(torch.diag(random_diag)).type(torch.FloatTensor)\n",
    "        block_list.append(hd_matrix)\n",
    "        current_seed += 1\n",
    "\n",
    "    remaining_rows = m - nb_full_blocks * d\n",
    "    if remaining_rows > 0:\n",
    "        if manual_seed:\n",
    "            torch.manual_seed(current_seed)\n",
    "        random_diag = torch.randint(-1, 2, (d,))\n",
    "        hd_matrix = hadamard_transform(torch.diag(random_diag)).type(torch.FloatTensor)\n",
    "        block_list.append(hd_matrix[0:remaining_rows])\n",
    "\n",
    "\n",
    "    return torch.vstack(block_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b39003a277504219aae118b3c90e71b0",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "### Gaussian Matrix Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1f4aecda-2c1a-451a-815a-e241c58e09a1",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": [
     {
      "fromCodePoint": 234,
      "marks": {
       "code": true
      },
      "toCodePoint": 243,
      "type": "marks"
     }
    ],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "Translated and simplified his original code written in TensorFlow to Torch. This can also be found in his PNG Kernel paper labels as G_ORT. The idea is to generate random matrices using Gaussian and do Gram-Schmidt Orthogonalization (linalg.qr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "b4bb590d393c4fe4b08246bf50a8d72f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1670543247953,
    "source_hash": "334aa7d3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_projection_gaussian(m, d, seed=0, manual_seed=False, scaling=0):\n",
    "\n",
    "    nb_full_blocks = int(m / d)\n",
    "    block_list = []\n",
    "    current_seed = seed\n",
    "    for _ in range(nb_full_blocks):\n",
    "        if manual_seed:\n",
    "            torch.manual_seed(current_seed)\n",
    "        unstructured_block = torch.randn(d, d)\n",
    "        q, _ = torch.linalg.qr(unstructured_block)\n",
    "        q = torch.transpose(q, 0, 1)\n",
    "        block_list.append(q)\n",
    "        current_seed += 1\n",
    "\n",
    "    remaining_rows = m - nb_full_blocks * d\n",
    "    if remaining_rows > 0:\n",
    "        if manual_seed:\n",
    "            torch.manual_seed(current_seed)\n",
    "        unstructured_block = torch.randn(d, d)\n",
    "        q, _ = torch.linalg.qr(unstructured_block)\n",
    "        q = torch.transpose(q, 0, 1)\n",
    "        block_list.append(q[0:remaining_rows])\n",
    "    \n",
    "    final_matrix = torch.vstack(block_list)\n",
    "    current_seed += 1\n",
    "\n",
    "    if scaling == 0:\n",
    "        if manual_seed:\n",
    "            torch.manual_seed(current_seed)\n",
    "        multiplier = torch.norm(torch.randn((m, d)), dim=1)\n",
    "    elif scaling == 1:\n",
    "        multiplier = torch.sqrt(torch.Tensor([float(d)]) * torch.ones((m)))\n",
    "    else:\n",
    "        raise ValueError(\"Scaling must be one of {0, 1}. Was %s\" % scaling)\n",
    "\n",
    "    final_matrix  = torch.matmul(torch.diag(multiplier), final_matrix)\n",
    "    assert final_matrix.shape[-1] == d\n",
    "    return final_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6187a370293742bbb5e0ef2d554790e8",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "# Image Classifier Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "cb73bd13032c47f487705c2cdb6e9094",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "62948b6c2acd4a7c95eaebfa9168cb84",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "37ff849cd9214493a1a764791fe2bcc2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4,
    "execution_start": 1670543247954,
    "source_hash": "c0a4d54f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, mlp_dim, performer=False):\n",
    "        super().__init__()\n",
    "\n",
    "        if performer is False:\n",
    "            self.layers = nn.ModuleList([\n",
    "                nn.ModuleList([\n",
    "                    Residual(PreNorm(dim, Attention(dim, heads=heads))),\n",
    "                    Residual(PreNorm(dim, FeedForward(dim, mlp_dim)))\n",
    "                ])\n",
    "                for _ in range(depth)\n",
    "            ])\n",
    "        else:\n",
    "            r, feature_map, projection_method, redraw = performer\n",
    "            self.layers = nn.ModuleList([\n",
    "                nn.ModuleList([\n",
    "                    Residual(PreNorm(dim, PerformerAttention(\n",
    "                        dim, heads, r, feature_map, projection_method, redraw\n",
    "                    ))),\n",
    "                    Residual(PreNorm(dim, FeedForward(dim, mlp_dim)))\n",
    "                ])\n",
    "                for _ in range(depth)\n",
    "            ])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x, mask=mask)\n",
    "            x = ff(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "17d99a34550148c8b0cd31a9a0ec50a0",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "70b418bc79534dad8acdc30b793707dc",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1670543247963,
    "source_hash": "e1d83b7f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, \n",
    "                 dim, depth, heads, mlp_dim, channels=3, performer=False):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = channels * patch_size ** 2\n",
    "        self.patch_size = patch_size\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.transformer = Transformer(dim, depth, heads, mlp_dim, performer=performer)\n",
    "        self.to_cls_token = nn.Identity()\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, mask=None):\n",
    "        p = self.patch_size\n",
    "        \n",
    "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        # x shape: (batch x n x p^2c)\n",
    "        \n",
    "        x = self.patch_to_embedding(x)\n",
    "        # linear map from p^2c to dim\n",
    "        # x shape: (batch x n x dim)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        # x shape (batch x n' x dim)\n",
    "\n",
    "        x += self.pos_embedding\n",
    "        x = self.transformer(x, mask)\n",
    "        x = self.to_cls_token(x[:, 0])\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a87f6c187b494c38b0333c424fe89915",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "# Training/Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "52c91a01a8e845b1ab67a4cc60a27c3e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 37,
    "execution_start": 1670543247972,
    "source_hash": "aab3122a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, data_loader, loss_history):\n",
    "    total_samples = len(data_loader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    epoch_timestamp = time.time()\n",
    "    for i, (data, target) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = F.log_softmax(model(data), dim=1)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            epoch_time = time.time() - epoch_timestamp\n",
    "            epoch_timestamp = time.time()\n",
    "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(total_samples) +\n",
    "                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader)) + '%)]  Loss: ' +\n",
    "                  '{:6.4f}'.format(loss.item()) + '  Epoch execution time: {:5.2f}'.format(epoch_time), 'seconds')\n",
    "            loss_history.append((time.time(), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "e80a3e2aa7b441cea27025761dfb9bf8",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4,
    "execution_start": 1670543248015,
    "source_hash": "af8a66ea",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, loss_history):\n",
    "    model.eval()\n",
    "    \n",
    "    total_samples = len(data_loader.dataset)\n",
    "    correct_samples = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            output = F.log_softmax(model(data), dim=1)\n",
    "            loss = F.nll_loss(output, target, reduction='sum')\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            correct_samples += pred.eq(target).sum()\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    loss_history.append((time.time(), avg_loss))\n",
    "\n",
    "    print('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\n",
    "          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\n",
    "          '{:5}'.format(total_samples) + ' (' +\n",
    "          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6edeba19eb354375b47152401bd809ca",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "51b922c12e1541b8b0eabf19ecdfa70d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7,
    "execution_start": 1670543248025,
    "source_hash": "4940bf16",
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "BATCH_SIZE_TRAIN = 100\n",
    "BATCH_SIZE_TEST = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "b3827b88b2cb4ee0aecd29a09149bffb",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 95,
    "execution_start": 1670543248035,
    "scrolled": false,
    "source_hash": "3831d853",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DOWNLOAD_PATH = r'C:\\Users\\zhang\\OneDrive\\Desktop\\Data Mining Project'\n",
    "\n",
    "transform_mnist = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(DOWNLOAD_PATH, train=True, download=True, transform=transform_mnist)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "test_set = torchvision.datasets.MNIST(DOWNLOAD_PATH, train=False, download=True, transform=transform_mnist)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE_TEST, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def format_history(train_loss_history, test_loss_history):\n",
    "    train_loss_history = np.array(train_loss_history).T\n",
    "    test_loss_history = np.array(test_loss_history).T\n",
    "    \n",
    "    start_time = train_loss_history[0, 0]\n",
    "    train_loss_history[0, :] = train_loss_history[0, :] - start_time\n",
    "    test_loss_history[0, :] = test_loss_history[0, :] - start_time\n",
    "\n",
    "    return train_loss_history, test_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d98ab577fae2459db22f2dd88e2f7e5a",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e746437110964353853796e1a845e943",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "### Data Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "782ec326ed214318b676724d6bc900e7",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1670543248177,
    "source_hash": "8ec9fad5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "patch_size = 4\n",
    "num_classes = 10\n",
    "channels = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ed4b04764a0e478a8ae96382478aabeb",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "### Model Related (ONLY CHANGE THIS PART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_id": "a5dc5bd9ab3b4e8289486be5e580f021",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1670543248177,
    "source_hash": "9e09c49c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "depth = 5\n",
    "dim = 32 # must be a power of 2 for HD projection\n",
    "heads = 4 # need to divide dim\n",
    "mlp_dim = 40\n",
    "r = 20\n",
    "projection_method = 'gaussian'\n",
    "# projection_method = 'HD' \n",
    "redraw = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "cd55b6836aa84a0484d9f7a42d9f7887",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## (a) Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "0c5e3d417eb74a3180f5ad811d258c2f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 158978,
    "execution_start": 1670543248178,
    "scrolled": true,
    "source_hash": "c2c2d678",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "[    0/60000 (  0%)]  Loss: 2.2595  Epoch execution time:  0.19 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 1.0442  Epoch execution time:  8.82 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.5691  Epoch execution time:  8.27 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.4381  Epoch execution time:  8.72 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.2039  Epoch execution time:  8.26 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.2355  Epoch execution time:  8.24 seconds\n",
      "\n",
      "Average test loss: 0.2157  Accuracy: 9318/10000 (93.18%)\n",
      "\n",
      "Epoch: 2\n",
      "[    0/60000 (  0%)]  Loss: 0.1821  Epoch execution time:  0.09 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.1394  Epoch execution time: 10.87 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.1607  Epoch execution time:  8.52 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.1561  Epoch execution time:  8.64 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.1897  Epoch execution time:  8.34 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1273  Epoch execution time:  8.44 seconds\n",
      "\n",
      "Average test loss: 0.1449  Accuracy: 9544/10000 (95.44%)\n",
      "\n",
      "Epoch: 3\n",
      "[    0/60000 (  0%)]  Loss: 0.0835  Epoch execution time:  0.08 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.1890  Epoch execution time:  9.48 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.1018  Epoch execution time:  8.84 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.2416  Epoch execution time:  8.84 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.2741  Epoch execution time:  9.39 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.2128  Epoch execution time:  9.19 seconds\n",
      "\n",
      "Average test loss: 0.1162  Accuracy: 9633/10000 (96.33%)\n",
      "\n",
      "Epoch: 4\n",
      "[    0/60000 (  0%)]  Loss: 0.0516  Epoch execution time:  0.09 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.1453  Epoch execution time: 10.97 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.2049  Epoch execution time:  9.57 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.0325  Epoch execution time: 11.07 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.0963  Epoch execution time:  9.08 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.0680  Epoch execution time: 10.43 seconds\n",
      "\n",
      "Average test loss: 0.1010  Accuracy: 9692/10000 (96.92%)\n",
      "\n",
      "Epoch: 5\n",
      "[    0/60000 (  0%)]  Loss: 0.0887  Epoch execution time:  0.09 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.0584  Epoch execution time:  8.73 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.1170  Epoch execution time:  8.88 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.0934  Epoch execution time:  9.08 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.0317  Epoch execution time:  9.72 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.0982  Epoch execution time:  9.39 seconds\n",
      "\n",
      "Average test loss: 0.0962  Accuracy: 9700/10000 (97.00%)\n",
      "\n",
      "Epoch: 6\n",
      "[    0/60000 (  0%)]  Loss: 0.1151  Epoch execution time:  0.13 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.1090  Epoch execution time: 11.29 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.0635  Epoch execution time: 11.26 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.1462  Epoch execution time: 11.20 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.0816  Epoch execution time:  9.93 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.0344  Epoch execution time:  8.55 seconds\n",
      "\n",
      "Average test loss: 0.1011  Accuracy: 9680/10000 (96.80%)\n",
      "\n",
      "Epoch: 7\n",
      "[    0/60000 (  0%)]  Loss: 0.1621  Epoch execution time:  0.08 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.1305  Epoch execution time:  8.61 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.0993  Epoch execution time:  8.58 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.0559  Epoch execution time:  8.48 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.0472  Epoch execution time:  8.50 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1498  Epoch execution time:  8.57 seconds\n",
      "\n",
      "Average test loss: 0.1142  Accuracy: 9647/10000 (96.47%)\n",
      "\n",
      "Epoch: 8\n",
      "[    0/60000 (  0%)]  Loss: 0.1147  Epoch execution time:  0.13 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.1353  Epoch execution time:  9.65 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.0672  Epoch execution time:  8.55 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.1041  Epoch execution time:  8.40 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.0461  Epoch execution time:  8.44 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.0386  Epoch execution time:  8.48 seconds\n",
      "\n",
      "Average test loss: 0.1150  Accuracy: 9645/10000 (96.45%)\n",
      "\n",
      "Epoch: 9\n",
      "[    0/60000 (  0%)]  Loss: 0.2434  Epoch execution time:  0.11 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.0251  Epoch execution time:  8.59 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.0664  Epoch execution time:  8.54 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.1278  Epoch execution time:  8.63 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.0172  Epoch execution time:  8.58 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.0532  Epoch execution time:  8.66 seconds\n",
      "\n",
      "Average test loss: 0.0921  Accuracy: 9716/10000 (97.16%)\n",
      "\n",
      "Epoch: 10\n",
      "[    0/60000 (  0%)]  Loss: 0.0679  Epoch execution time:  0.08 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.0798  Epoch execution time: 10.57 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.1098  Epoch execution time:  8.83 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.0858  Epoch execution time:  8.49 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.1999  Epoch execution time:  9.70 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.0712  Epoch execution time:  9.39 seconds\n",
      "\n",
      "Average test loss: 0.1064  Accuracy: 9686/10000 (96.86%)\n",
      "\n",
      "Execution time: 593.75 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model = ViT(\n",
    "    image_size=image_size, patch_size=patch_size, num_classes=num_classes, \n",
    "    channels=channels, dim=dim, depth=depth, heads=heads, mlp_dim=mlp_dim\n",
    ")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "train_loss_history_sm, test_loss_history_sm = [], []\n",
    "\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    print('Epoch:', epoch)\n",
    "    train_epoch(model, optimizer, train_loader, train_loss_history_sm)\n",
    "    evaluate(model, test_loader, test_loss_history_sm)\n",
    "\n",
    "    \n",
    "train_loss_history_sm, test_loss_history_sm = format_history(train_loss_history_sm, test_loss_history_sm)\n",
    "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0e9449a516104e9d995d0ace5a7c37d7",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## (b) Performer-ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "[    0/60000 (  0%)]  Loss: 2.3095  Epoch execution time:  0.15 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 1.9929  Epoch execution time: 10.86 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 1.5954  Epoch execution time: 10.94 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 1.1199  Epoch execution time: 10.91 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.9635  Epoch execution time: 10.99 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 1.1791  Epoch execution time: 11.06 seconds\n",
      "\n",
      "Average test loss: 0.6976  Accuracy: 7589/10000 (75.89%)\n",
      "\n",
      "Epoch: 2\n",
      "[    0/60000 (  0%)]  Loss: 0.6588  Epoch execution time:  0.11 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.4884  Epoch execution time: 10.99 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.5435  Epoch execution time: 10.93 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.4407  Epoch execution time: 10.81 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.2770  Epoch execution time: 10.97 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.2923  Epoch execution time: 10.86 seconds\n",
      "\n",
      "Average test loss: 0.3367  Accuracy: 8928/10000 (89.28%)\n",
      "\n",
      "Epoch: 3\n",
      "[    0/60000 (  0%)]  Loss: 0.3267  Epoch execution time:  0.12 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.4615  Epoch execution time: 10.84 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.2258  Epoch execution time: 10.89 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.2335  Epoch execution time: 10.79 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.2598  Epoch execution time: 11.63 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.2069  Epoch execution time: 11.90 seconds\n",
      "\n",
      "Average test loss: 0.2458  Accuracy: 9209/10000 (92.09%)\n",
      "\n",
      "Epoch: 4\n",
      "[    0/60000 (  0%)]  Loss: 0.1875  Epoch execution time:  0.11 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.2806  Epoch execution time: 10.84 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.3406  Epoch execution time: 10.79 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.2979  Epoch execution time: 10.75 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.1072  Epoch execution time: 10.82 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1608  Epoch execution time: 10.80 seconds\n",
      "\n",
      "Average test loss: 0.2060  Accuracy: 9329/10000 (93.29%)\n",
      "\n",
      "Epoch: 5\n",
      "[    0/60000 (  0%)]  Loss: 0.1544  Epoch execution time:  0.11 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.0711  Epoch execution time: 10.90 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.1446  Epoch execution time: 11.37 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.1702  Epoch execution time: 12.22 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.2131  Epoch execution time: 10.86 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1137  Epoch execution time: 10.93 seconds\n",
      "\n",
      "Average test loss: 0.1722  Accuracy: 9471/10000 (94.71%)\n",
      "\n",
      "Epoch: 6\n",
      "[    0/60000 (  0%)]  Loss: 0.1637  Epoch execution time:  0.11 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.2305  Epoch execution time: 10.95 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.2041  Epoch execution time: 11.00 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.1844  Epoch execution time: 11.01 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.2163  Epoch execution time: 10.83 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.2553  Epoch execution time: 10.82 seconds\n",
      "\n",
      "Average test loss: 0.1525  Accuracy: 9530/10000 (95.30%)\n",
      "\n",
      "Epoch: 7\n",
      "[    0/60000 (  0%)]  Loss: 0.1471  Epoch execution time:  0.14 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.0497  Epoch execution time: 12.14 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.1305  Epoch execution time: 10.81 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.1064  Epoch execution time: 10.84 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.1185  Epoch execution time: 10.83 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.0744  Epoch execution time: 10.87 seconds\n",
      "\n",
      "Average test loss: 0.1720  Accuracy: 9452/10000 (94.52%)\n",
      "\n",
      "Epoch: 8\n",
      "[    0/60000 (  0%)]  Loss: 0.3000  Epoch execution time:  0.11 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.0828  Epoch execution time: 10.87 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.1017  Epoch execution time: 10.82 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.1779  Epoch execution time: 10.87 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.0370  Epoch execution time: 10.82 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1307  Epoch execution time: 12.58 seconds\n",
      "\n",
      "Average test loss: 0.1253  Accuracy: 9592/10000 (95.92%)\n",
      "\n",
      "Epoch: 9\n",
      "[    0/60000 (  0%)]  Loss: 0.1010  Epoch execution time:  0.11 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.1024  Epoch execution time: 10.85 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.1683  Epoch execution time: 10.94 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.2396  Epoch execution time: 10.88 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.2513  Epoch execution time: 10.85 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1498  Epoch execution time: 10.95 seconds\n",
      "\n",
      "Average test loss: 0.1301  Accuracy: 9606/10000 (96.06%)\n",
      "\n",
      "Epoch: 10\n",
      "[    0/60000 (  0%)]  Loss: 0.2392  Epoch execution time:  0.11 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.1115  Epoch execution time: 10.84 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.1261  Epoch execution time: 10.81 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.1048  Epoch execution time: 12.67 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.0802  Epoch execution time: 13.33 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1218  Epoch execution time: 13.23 seconds\n",
      "\n",
      "Average test loss: 0.1175  Accuracy: 9615/10000 (96.15%)\n",
      "\n",
      "Execution time: 714.73 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "numerical_stabilizer=0.001\n",
    "model = ViT(\n",
    "    image_size=image_size, patch_size=patch_size, num_classes=num_classes, \n",
    "    channels=channels, dim=dim, depth=depth, heads=heads, mlp_dim=mlp_dim, \n",
    "    performer=(r, lambda x: nn.ReLU()(x) + numerical_stabilizer, 'gaussian', redraw)\n",
    ")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "train_loss_history_relu, test_loss_history_relu  = [], []\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    print('Epoch:', epoch)\n",
    "    train_epoch(model, optimizer, train_loader, train_loss_history_relu)\n",
    "    evaluate(model, test_loader, test_loss_history_relu)\n",
    "    \n",
    "train_loss_history_relu, test_loss_history_relu = format_history(train_loss_history_relu, test_loss_history_relu)\n",
    "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cell_id": "9bc412db39d54dd599e2255f2bce4d0d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 114896,
    "execution_start": 1670543120608,
    "scrolled": true,
    "source_hash": "954dfaa4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "[    0/60000 (  0%)]  Loss: 2.3868  Epoch execution time:  0.12 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 1.9273  Epoch execution time: 11.22 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 1.6859  Epoch execution time: 11.03 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 1.4817  Epoch execution time: 11.24 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.8271  Epoch execution time: 11.11 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.9964  Epoch execution time: 11.07 seconds\n",
      "\n",
      "Average test loss: 0.6641  Accuracy: 7761/10000 (77.61%)\n",
      "\n",
      "Epoch: 2\n",
      "[    0/60000 (  0%)]  Loss: 0.5802  Epoch execution time:  0.13 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.6108  Epoch execution time: 12.02 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.4780  Epoch execution time: 11.13 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.4099  Epoch execution time: 11.07 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.4513  Epoch execution time: 11.11 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.4500  Epoch execution time: 11.11 seconds\n",
      "\n",
      "Average test loss: 0.3929  Accuracy: 8698/10000 (86.98%)\n",
      "\n",
      "Epoch: 3\n",
      "[    0/60000 (  0%)]  Loss: 0.2374  Epoch execution time:  0.11 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.3010  Epoch execution time: 11.13 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.4527  Epoch execution time: 11.02 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.4228  Epoch execution time: 11.05 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.2846  Epoch execution time: 11.25 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.2065  Epoch execution time: 12.62 seconds\n",
      "\n",
      "Average test loss: 0.3427  Accuracy: 8845/10000 (88.45%)\n",
      "\n",
      "Epoch: 4\n",
      "[    0/60000 (  0%)]  Loss: 0.3700  Epoch execution time:  0.11 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.2291  Epoch execution time: 11.10 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.1507  Epoch execution time: 11.29 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.3223  Epoch execution time: 11.05 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.2455  Epoch execution time: 11.17 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1424  Epoch execution time: 11.06 seconds\n",
      "\n",
      "Average test loss: 0.2519  Accuracy: 9203/10000 (92.03%)\n",
      "\n",
      "Epoch: 5\n",
      "[    0/60000 (  0%)]  Loss: 0.2032  Epoch execution time:  0.12 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.2684  Epoch execution time: 11.21 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.1334  Epoch execution time: 11.86 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.1095  Epoch execution time: 12.13 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.3616  Epoch execution time: 11.07 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.3089  Epoch execution time: 11.17 seconds\n",
      "\n",
      "Average test loss: 0.2393  Accuracy: 9236/10000 (92.36%)\n",
      "\n",
      "Epoch: 6\n",
      "[    0/60000 (  0%)]  Loss: 0.2454  Epoch execution time:  0.11 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.3020  Epoch execution time: 11.16 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.2935  Epoch execution time: 11.13 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.1778  Epoch execution time: 11.20 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.2104  Epoch execution time: 11.08 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1948  Epoch execution time: 11.10 seconds\n",
      "\n",
      "Average test loss: 0.1689  Accuracy: 9475/10000 (94.75%)\n",
      "\n",
      "Epoch: 7\n",
      "[    0/60000 (  0%)]  Loss: 0.1190  Epoch execution time:  0.14 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.1479  Epoch execution time: 11.81 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.2080  Epoch execution time: 11.10 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.2438  Epoch execution time: 11.09 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.1640  Epoch execution time: 11.14 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.0870  Epoch execution time: 11.02 seconds\n",
      "\n",
      "Average test loss: 0.1570  Accuracy: 9483/10000 (94.83%)\n",
      "\n",
      "Epoch: 8\n",
      "[    0/60000 (  0%)]  Loss: 0.1118  Epoch execution time:  0.10 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.2631  Epoch execution time: 11.16 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.1421  Epoch execution time: 11.15 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.1783  Epoch execution time: 11.15 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.3147  Epoch execution time: 11.83 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.2388  Epoch execution time: 12.10 seconds\n",
      "\n",
      "Average test loss: 0.1633  Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Epoch: 9\n",
      "[    0/60000 (  0%)]  Loss: 0.1033  Epoch execution time:  0.11 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.0746  Epoch execution time: 11.06 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.1458  Epoch execution time: 11.23 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.1569  Epoch execution time: 11.15 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.1679  Epoch execution time: 11.13 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1047  Epoch execution time: 11.24 seconds\n",
      "\n",
      "Average test loss: 0.1381  Accuracy: 9547/10000 (95.47%)\n",
      "\n",
      "Epoch: 10\n",
      "[    0/60000 (  0%)]  Loss: 0.0687  Epoch execution time:  0.11 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.2209  Epoch execution time: 11.10 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.1184  Epoch execution time: 12.45 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.2051  Epoch execution time: 11.56 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.1422  Epoch execution time: 11.16 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.3047  Epoch execution time: 11.04 seconds\n",
      "\n",
      "Average test loss: 0.1650  Accuracy: 9495/10000 (94.95%)\n",
      "\n",
      "Execution time: 724.04 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "numerical_stabilizer=0.001\n",
    "model = ViT(\n",
    "    image_size=image_size, patch_size=patch_size, num_classes=num_classes, \n",
    "    channels=channels, dim=dim, depth=depth, heads=heads, mlp_dim=mlp_dim, \n",
    "    performer=(r, lambda x: nn.ReLU()(x) + numerical_stabilizer, 'HD', redraw)\n",
    ")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "train_loss_history_relu, test_loss_history_relu  = [], []\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    print('Epoch:', epoch)\n",
    "    train_epoch(model, optimizer, train_loader, train_loss_history_relu)\n",
    "    evaluate(model, test_loader, test_loss_history_relu)\n",
    "    \n",
    "train_loss_history_relu, test_loss_history_relu = format_history(train_loss_history_relu, test_loss_history_relu)\n",
    "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7e5f980c56d2408a9f834a18aeea7e4a",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## (c) Performer Quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cell_id": "e511353612b646b487be8c033d6c6011",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 1234723,
    "execution_start": 1670381596076,
    "scrolled": true,
    "source_hash": "44a28f71",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "[    0/60000 (  0%)]  Loss: 2.3372  Epoch execution time:  0.12 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 1.3987  Epoch execution time: 11.09 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.8776  Epoch execution time: 11.11 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.4961  Epoch execution time: 11.16 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.4585  Epoch execution time: 11.17 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.3387  Epoch execution time: 11.07 seconds\n",
      "\n",
      "Average test loss: 0.2637  Accuracy: 9147/10000 (91.47%)\n",
      "\n",
      "Epoch: 2\n",
      "[    0/60000 (  0%)]  Loss: 0.2271  Epoch execution time:  0.12 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.2088  Epoch execution time: 11.38 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.1802  Epoch execution time: 11.11 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.2114  Epoch execution time: 11.15 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.1172  Epoch execution time: 11.08 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1826  Epoch execution time: 11.19 seconds\n",
      "\n",
      "Average test loss: 0.1824  Accuracy: 9413/10000 (94.13%)\n",
      "\n",
      "Epoch: 3\n",
      "[    0/60000 (  0%)]  Loss: 0.2378  Epoch execution time:  0.11 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.1094  Epoch execution time: 11.19 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.1355  Epoch execution time: 11.16 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.0895  Epoch execution time: 11.23 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.3345  Epoch execution time: 12.18 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.0494  Epoch execution time: 13.50 seconds\n",
      "\n",
      "Average test loss: 0.1202  Accuracy: 9592/10000 (95.92%)\n",
      "\n",
      "Epoch: 4\n",
      "[    0/60000 (  0%)]  Loss: 0.1465  Epoch execution time:  0.14 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.0854  Epoch execution time: 11.82 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.1943  Epoch execution time: 11.15 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.0954  Epoch execution time: 11.08 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.1655  Epoch execution time: 11.16 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1463  Epoch execution time: 11.23 seconds\n",
      "\n",
      "Average test loss: 0.1029  Accuracy: 9672/10000 (96.72%)\n",
      "\n",
      "Epoch: 5\n",
      "[    0/60000 (  0%)]  Loss: 0.0348  Epoch execution time:  0.11 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.2039  Epoch execution time: 11.30 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.1686  Epoch execution time: 12.57 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.0708  Epoch execution time: 11.17 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.1148  Epoch execution time: 11.13 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1099  Epoch execution time: 11.08 seconds\n",
      "\n",
      "Average test loss: 0.1026  Accuracy: 9673/10000 (96.73%)\n",
      "\n",
      "Epoch: 6\n",
      "[    0/60000 (  0%)]  Loss: 0.0511  Epoch execution time:  0.11 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.1551  Epoch execution time: 11.18 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.0644  Epoch execution time: 11.10 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.0672  Epoch execution time: 11.17 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.0253  Epoch execution time: 11.13 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1529  Epoch execution time: 11.07 seconds\n",
      "\n",
      "Average test loss: 0.1056  Accuracy: 9672/10000 (96.72%)\n",
      "\n",
      "Epoch: 7\n",
      "[    0/60000 (  0%)]  Loss: 0.2180  Epoch execution time:  0.11 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.0351  Epoch execution time: 11.12 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.0913  Epoch execution time: 11.08 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.1182  Epoch execution time: 11.11 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.0571  Epoch execution time: 11.08 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.0631  Epoch execution time: 11.12 seconds\n",
      "\n",
      "Average test loss: 0.1014  Accuracy: 9678/10000 (96.78%)\n",
      "\n",
      "Epoch: 8\n",
      "[    0/60000 (  0%)]  Loss: 0.0301  Epoch execution time:  0.11 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.1115  Epoch execution time: 11.12 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.0414  Epoch execution time: 11.11 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.0247  Epoch execution time: 11.16 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.0394  Epoch execution time: 12.65 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1053  Epoch execution time: 11.08 seconds\n",
      "\n",
      "Average test loss: 0.0848  Accuracy: 9740/10000 (97.40%)\n",
      "\n",
      "Epoch: 9\n",
      "[    0/60000 (  0%)]  Loss: 0.0570  Epoch execution time:  0.12 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.0454  Epoch execution time: 11.13 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.0409  Epoch execution time: 11.14 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.0466  Epoch execution time: 11.10 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.0081  Epoch execution time: 11.11 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.0894  Epoch execution time: 11.14 seconds\n",
      "\n",
      "Average test loss: 0.0774  Accuracy: 9746/10000 (97.46%)\n",
      "\n",
      "Epoch: 10\n",
      "[    0/60000 (  0%)]  Loss: 0.0993  Epoch execution time:  0.13 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.0439  Epoch execution time: 11.38 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.0835  Epoch execution time: 12.42 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.1195  Epoch execution time: 11.04 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.1148  Epoch execution time: 11.10 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.0338  Epoch execution time: 11.07 seconds\n",
      "\n",
      "Average test loss: 0.0789  Accuracy: 9757/10000 (97.57%)\n",
      "\n",
      "Execution time: 726.11 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = ViT(\n",
    "    image_size=image_size, patch_size=patch_size, num_classes=num_classes, \n",
    "    channels=channels, dim=dim, depth=depth, heads=heads, mlp_dim=mlp_dim,\n",
    "    performer=(r, lambda x: torch.pow(x, 2), 'gaussian', redraw)\n",
    ")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "train_loss_history_x2, test_loss_history_x2  = [], []\n",
    "\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    print('Epoch:', epoch)\n",
    "    train_epoch(model, optimizer, train_loader, train_loss_history_x2)\n",
    "    evaluate(model, test_loader, test_loss_history_x2)\n",
    "\n",
    "train_loss_history_x2, test_loss_history_x2 = format_history(train_loss_history_x2, test_loss_history_x2)\n",
    "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "[    0/60000 (  0%)]  Loss: 2.3494  Epoch execution time:  0.14 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.9343  Epoch execution time: 11.46 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.7108  Epoch execution time: 11.39 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.2041  Epoch execution time: 11.95 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.3136  Epoch execution time: 13.45 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.3046  Epoch execution time: 11.97 seconds\n",
      "\n",
      "Average test loss: 0.2578  Accuracy: 9193/10000 (91.93%)\n",
      "\n",
      "Epoch: 2\n",
      "[    0/60000 (  0%)]  Loss: 0.1734  Epoch execution time:  0.12 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.3162  Epoch execution time: 11.39 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.2639  Epoch execution time: 11.78 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.3331  Epoch execution time: 11.57 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.1649  Epoch execution time: 11.38 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1699  Epoch execution time: 11.34 seconds\n",
      "\n",
      "Average test loss: 0.1969  Accuracy: 9376/10000 (93.76%)\n",
      "\n",
      "Epoch: 3\n",
      "[    0/60000 (  0%)]  Loss: 0.1902  Epoch execution time:  0.11 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.1617  Epoch execution time: 11.41 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.1251  Epoch execution time: 11.43 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.2793  Epoch execution time: 12.77 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.1846  Epoch execution time: 13.46 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1439  Epoch execution time: 11.61 seconds\n",
      "\n",
      "Average test loss: 0.1390  Accuracy: 9565/10000 (95.65%)\n",
      "\n",
      "Epoch: 4\n",
      "[    0/60000 (  0%)]  Loss: 0.0711  Epoch execution time:  0.11 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.0772  Epoch execution time: 11.46 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.0898  Epoch execution time: 11.86 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.1090  Epoch execution time: 11.52 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.1614  Epoch execution time: 11.53 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1821  Epoch execution time: 11.39 seconds\n",
      "\n",
      "Average test loss: 0.1291  Accuracy: 9590/10000 (95.90%)\n",
      "\n",
      "Epoch: 5\n",
      "[    0/60000 (  0%)]  Loss: 0.0427  Epoch execution time:  0.14 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.1407  Epoch execution time: 13.68 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.0860  Epoch execution time: 13.51 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.3586  Epoch execution time: 13.29 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.0827  Epoch execution time: 11.41 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.0613  Epoch execution time: 11.48 seconds\n",
      "\n",
      "Average test loss: 0.1665  Accuracy: 9484/10000 (94.84%)\n",
      "\n",
      "Epoch: 6\n",
      "[    0/60000 (  0%)]  Loss: 0.1508  Epoch execution time:  0.13 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.0465  Epoch execution time: 11.44 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.0831  Epoch execution time: 11.41 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.1769  Epoch execution time: 11.45 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.0710  Epoch execution time: 12.86 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1548  Epoch execution time: 11.57 seconds\n",
      "\n",
      "Average test loss: 0.1090  Accuracy: 9656/10000 (96.56%)\n",
      "\n",
      "Epoch: 7\n",
      "[    0/60000 (  0%)]  Loss: 0.0664  Epoch execution time:  0.11 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.1049  Epoch execution time: 11.47 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.1376  Epoch execution time: 11.52 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.0791  Epoch execution time: 11.39 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.1968  Epoch execution time: 11.46 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1191  Epoch execution time: 11.44 seconds\n",
      "\n",
      "Average test loss: 0.0911  Accuracy: 9701/10000 (97.01%)\n",
      "\n",
      "Epoch: 8\n",
      "[    0/60000 (  0%)]  Loss: 0.0340  Epoch execution time:  0.13 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.0250  Epoch execution time: 12.02 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.0813  Epoch execution time: 12.45 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.1245  Epoch execution time: 11.49 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.1119  Epoch execution time: 11.40 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1600  Epoch execution time: 11.52 seconds\n",
      "\n",
      "Average test loss: 0.1081  Accuracy: 9666/10000 (96.66%)\n",
      "\n",
      "Epoch: 9\n",
      "[    0/60000 (  0%)]  Loss: 0.1305  Epoch execution time:  0.11 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.0406  Epoch execution time: 11.47 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.0710  Epoch execution time: 11.56 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.0692  Epoch execution time: 11.44 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.0296  Epoch execution time: 11.41 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.0691  Epoch execution time: 12.05 seconds\n",
      "\n",
      "Average test loss: 0.0936  Accuracy: 9708/10000 (97.08%)\n",
      "\n",
      "Epoch: 10\n",
      "[    0/60000 (  0%)]  Loss: 0.1314  Epoch execution time:  0.14 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.0319  Epoch execution time: 11.46 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.1232  Epoch execution time: 11.45 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.0449  Epoch execution time: 11.48 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.0498  Epoch execution time: 11.42 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1359  Epoch execution time: 11.48 seconds\n",
      "\n",
      "Average test loss: 0.0987  Accuracy: 9695/10000 (96.95%)\n",
      "\n",
      "Execution time: 750.61 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = ViT(\n",
    "    image_size=image_size, patch_size=patch_size, num_classes=num_classes, \n",
    "    channels=channels, dim=dim, depth=depth, heads=heads, mlp_dim=mlp_dim,\n",
    "    performer=(r, lambda x: torch.pow(x, 2), 'HD', redraw)\n",
    ")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "train_loss_history_x2, test_loss_history_x2  = [], []\n",
    "\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    print('Epoch:', epoch)\n",
    "    train_epoch(model, optimizer, train_loader, train_loss_history_x2)\n",
    "    evaluate(model, test_loader, test_loss_history_x2)\n",
    "\n",
    "train_loss_history_x2, test_loss_history_x2 = format_history(train_loss_history_x2, test_loss_history_x2)\n",
    "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "df9d8ac877634c5199e5f1ed8c2fb7e4",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## (d) Performer x^4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cell_id": "d34bc330a3904f24a75da815e9d77b00",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 1309019,
    "execution_start": 1670382830803,
    "scrolled": true,
    "source_hash": "6257add4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "[    0/60000 (  0%)]  Loss: 2.4367  Epoch execution time:  0.14 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.9346  Epoch execution time: 11.96 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.5093  Epoch execution time: 11.85 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.3435  Epoch execution time: 13.62 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.2104  Epoch execution time: 11.97 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.3569  Epoch execution time: 11.80 seconds\n",
      "\n",
      "Average test loss: 0.2611  Accuracy: 9162/10000 (91.62%)\n",
      "\n",
      "Epoch: 2\n",
      "[    0/60000 (  0%)]  Loss: 0.2220  Epoch execution time:  0.12 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.1870  Epoch execution time: 11.93 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.1581  Epoch execution time: 11.85 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.1895  Epoch execution time: 11.97 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.2095  Epoch execution time: 11.92 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.2389  Epoch execution time: 11.87 seconds\n",
      "\n",
      "Average test loss: 0.1755  Accuracy: 9460/10000 (94.60%)\n",
      "\n",
      "Epoch: 3\n",
      "[    0/60000 (  0%)]  Loss: 0.1412  Epoch execution time:  0.12 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.1548  Epoch execution time: 12.04 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.0975  Epoch execution time: 11.94 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.0622  Epoch execution time: 11.83 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.2003  Epoch execution time: 11.97 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1220  Epoch execution time: 11.85 seconds\n",
      "\n",
      "Average test loss: 0.1414  Accuracy: 9561/10000 (95.61%)\n",
      "\n",
      "Epoch: 4\n",
      "[    0/60000 (  0%)]  Loss: 0.1209  Epoch execution time:  0.12 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.2395  Epoch execution time: 11.86 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.0719  Epoch execution time: 11.92 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.0940  Epoch execution time: 12.41 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.0515  Epoch execution time: 13.16 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1704  Epoch execution time: 11.93 seconds\n",
      "\n",
      "Average test loss: 0.1306  Accuracy: 9592/10000 (95.92%)\n",
      "\n",
      "Epoch: 5\n",
      "[    0/60000 (  0%)]  Loss: 0.1375  Epoch execution time:  0.12 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.1417  Epoch execution time: 11.87 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.1217  Epoch execution time: 11.94 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.0454  Epoch execution time: 11.93 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.0788  Epoch execution time: 11.89 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.0598  Epoch execution time: 11.97 seconds\n",
      "\n",
      "Average test loss: 0.1115  Accuracy: 9658/10000 (96.58%)\n",
      "\n",
      "Epoch: 6\n",
      "[    0/60000 (  0%)]  Loss: 0.1074  Epoch execution time:  0.15 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.1356  Epoch execution time: 13.31 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.0737  Epoch execution time: 11.94 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.0781  Epoch execution time: 11.96 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.0693  Epoch execution time: 11.89 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1345  Epoch execution time: 11.85 seconds\n",
      "\n",
      "Average test loss: 0.1113  Accuracy: 9662/10000 (96.62%)\n",
      "\n",
      "Epoch: 7\n",
      "[    0/60000 (  0%)]  Loss: 0.1739  Epoch execution time:  0.12 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.1853  Epoch execution time: 11.92 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.0894  Epoch execution time: 11.98 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.0369  Epoch execution time: 11.92 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.0326  Epoch execution time: 13.69 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.0759  Epoch execution time: 14.70 seconds\n",
      "\n",
      "Average test loss: 0.1213  Accuracy: 9631/10000 (96.31%)\n",
      "\n",
      "Epoch: 8\n",
      "[    0/60000 (  0%)]  Loss: 0.1306  Epoch execution time:  0.13 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.1036  Epoch execution time: 11.89 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.0452  Epoch execution time: 11.88 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.1059  Epoch execution time: 11.86 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.0402  Epoch execution time: 11.86 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.0220  Epoch execution time: 11.94 seconds\n",
      "\n",
      "Average test loss: 0.0944  Accuracy: 9704/10000 (97.04%)\n",
      "\n",
      "Epoch: 9\n",
      "[    0/60000 (  0%)]  Loss: 0.1201  Epoch execution time:  0.14 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.0238  Epoch execution time: 13.60 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.0769  Epoch execution time: 11.92 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.0548  Epoch execution time: 12.24 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.0444  Epoch execution time: 11.87 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.1078  Epoch execution time: 11.88 seconds\n",
      "\n",
      "Average test loss: 0.0952  Accuracy: 9711/10000 (97.11%)\n",
      "\n",
      "Epoch: 10\n",
      "[    0/60000 (  0%)]  Loss: 0.0206  Epoch execution time:  0.12 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.0758  Epoch execution time: 11.88 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.0516  Epoch execution time: 11.94 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.0143  Epoch execution time: 11.98 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.0494  Epoch execution time: 13.78 seconds\n",
      "[50000/60000 ( 83%)]  Loss: 0.0441  Epoch execution time: 12.42 seconds\n",
      "\n",
      "Average test loss: 0.0914  Accuracy: 9731/10000 (97.31%)\n",
      "\n",
      "Execution time: 784.93 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = ViT(\n",
    "    image_size=image_size, patch_size=patch_size, num_classes=num_classes, \n",
    "    channels=channels, dim=dim, depth=depth, heads=heads, mlp_dim=mlp_dim,\n",
    "    performer=(r, lambda x: torch.pow(x, 4), 'gaussian', redraw)\n",
    ")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "train_loss_history_x4, test_loss_history_x4  = [], []\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    print('Epoch:', epoch)\n",
    "    train_epoch(model, optimizer, train_loader, train_loss_history_x4)\n",
    "    evaluate(model, test_loader, test_loss_history_x4)\n",
    "\n",
    "train_loss_history_x4, test_loss_history_x4 = format_history(train_loss_history_x4, test_loss_history_x4)\n",
    "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "[    0/60000 (  0%)]  Loss: 2.3123  Epoch execution time:  0.13 seconds\n",
      "[10000/60000 ( 17%)]  Loss: 0.6563  Epoch execution time: 12.56 seconds\n",
      "[20000/60000 ( 33%)]  Loss: 0.4731  Epoch execution time: 12.48 seconds\n",
      "[30000/60000 ( 50%)]  Loss: 0.2371  Epoch execution time: 12.59 seconds\n",
      "[40000/60000 ( 67%)]  Loss: 0.2853  Epoch execution time: 12.68 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-84fb56bc9362>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_EPOCHS\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss_history_x4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loss_history_x4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-9c354cd73570>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(model, optimizer, data_loader, loss_history)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             )\n\u001b[1;32m--> 487\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = ViT(\n",
    "    image_size=image_size, patch_size=patch_size, num_classes=num_classes, \n",
    "    channels=channels, dim=dim, depth=depth, heads=heads, mlp_dim=mlp_dim,\n",
    "    performer=(r, lambda x: torch.pow(x, 4),'HD', redraw)\n",
    ")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "train_loss_history_x4, test_loss_history_x4  = [], []\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    print('Epoch:', epoch)\n",
    "    train_epoch(model, optimizer, train_loader, train_loss_history_x4)\n",
    "    evaluate(model, test_loader, test_loss_history_x4)\n",
    "\n",
    "train_loss_history_x4, test_loss_history_x4 = format_history(train_loss_history_x4, test_loss_history_x4)\n",
    "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_loss = pd.DataFrame({\n",
    "    ('softmax', 'time'): train_loss_history_sm[0, :],\n",
    "    ('softmax', 'loss'): train_loss_history_sm[1, :],\n",
    "    ('ReLU', 'time'): train_loss_history_relu[0, :],\n",
    "    ('ReLU', 'loss'): train_loss_history_relu[1, :],\n",
    "    ('x^2', 'time'): train_loss_history_x2[0, :],\n",
    "    ('x^2', 'loss'): train_loss_history_x2[1, :],\n",
    "    ('x^4', 'time'): train_loss_history_x4[0, :], \n",
    "    ('x^4', 'loss'): train_loss_history_x4[1, :]\n",
    "}).T\n",
    "\n",
    "test_loss = pd.DataFrame({\n",
    "    ('softmax', 'time'): test_loss_history_sm[0, :],\n",
    "    ('softmax', 'loss'): test_loss_history_sm[1, :],\n",
    "    ('ReLU', 'time'): test_loss_history_relu[0, :],\n",
    "    ('ReLU', 'loss'): test_loss_history_relu[1, :],\n",
    "    ('x^2', 'time'): test_loss_history_x2[0, :],\n",
    "    ('x^2', 'loss'): test_loss_history_x2[1, :],\n",
    "    ('x^4', 'time'): test_loss_history_x4[0, :], \n",
    "    ('x^4', 'loss'): test_loss_history_x4[1, :]\n",
    "}).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss.to_csv('train loss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss.to_csv('test loss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "8d4011614eb041d1bb637048e3ed7f99",
  "kernelspec": {
   "display_name": "goodenv",
   "language": "python",
   "name": "goodenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
